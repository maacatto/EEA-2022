---
title: "Regresión Lineal Múltiple II"
author: "Juan Barriola, Azul Villanueva y Franco Mastelli"
date: "24 de septiembre de 2022"
output:
  html_notebook:
    theme: spacelab
    toc: yes
    toc_float: yes
    df_print: paged
---

<style type="text/css">
div.main-container {
  max-width: 1600px;
  margin-left: auto;
  margin-right: auto;
}
</style>

## Diagnóstico y Evaluación de Modelos de Regresión Lineal Múltiple

### Dataset 

Vamos a trabajar con el subconjunto de datos que se utilizó en la clase anterior, precio de venta en dólares de las propiedades en Capital Federal reportadas por la empresa [Properati Argentina](https://www.properati.com.ar/). 

Nuestro objetivo es evaluar modelos de regresión lineal múltiple que buscan explicar el precio de venta en dólares de dichas propiedades.

Es decir, evaluamos los siguientes modelos para el precio:

$precio = \beta_0 +\beta_1X_1+\beta_2X_2+...+\epsilon$

```{r, warning=F, message=F}
library(tidyverse)
library(tidymodels)
library(gridExtra)
```

#### Levantamos Dataset 

```{r}
# levantamos dataset preprocesado
datos_properati <- read.csv("../clase 6/properati_preprocesado_2022.csv")
# creamos nueva variable de superficie descubierta
datos_properati = datos_properati %>%
  mutate(surface_uncovered = surface_total - surface_covered)
```

### Partición del dataset en train y test

En este caso para evaluar los modelos vamos a realizar una partición entre dataset de entrenamiento (75%) y testeo (25%) usando la función `initial_split` del paquete [rsample](https://rsample.tidymodels.org/) de tidymodels.

```{r}
# fijamos semilla
set.seed(22)
# Partición Train y Test, indicando proporción
train_test <- initial_split(datos_properati, prop = 0.75)
train_data <- training(train_test)
test_data <- testing(train_test)
# vemos las dimensiones de cada partición
train_data %>%
  dim_desc() 
test_data %>%
  dim_desc() 
```

**Nota sobre Data Leakage**

Vamos a traer el dataset de clasificación de barrios para poder utilizarlo en los modelos. Cabe aclarar que es posible utilizar esta categorización en ambos datasets ya que cuando generamos la clasificación usamos los datos de training. Si para generar una clasificación usáramos precios por metro cuadrado de todo el dataset antes de particionar (train y test) estaríamos incurriendo en **data leakage**, dado que estaríamos usando la variable a predecir del set de testing en el ajuste del modelo. 

```{r}
# levantamos dataset de tipo de barrio
AVG_pxm2_l3 <- read.csv("../clase 6/AVG_pxm2_l3.csv")
# unimos la clasificación de tipo de barrio creada en la clase anterior al dataset
train_data = train_data %>% left_join(AVG_pxm2_l3[c("l3","tipo_barrio")], by = 'l3') 
test_data = test_data %>% left_join(AVG_pxm2_l3[c("l3","tipo_barrio")], by = 'l3') 
head(train_data)
```

El data leakage (o fuga de datos) es el uso de información en el proceso de entrenamiento del modelo que no se esperaría que estuviera disponible en el momento de la predicción, lo que hace que las métricas sobreestimen la utilidad del modelo. 

Se produce cuando se utiliza información externa al conjunto de datos de entrenamiento para crear el modelo. Esta información adicional puede permitir que el modelo aprenda o sepa algo que de otro modo no sabría y, a su vez, invalidará el rendimiento estimado del modo que se está construyendo.

### Ajuste

Habíamos realizado cuatro modelos distintos para tratar de explicar el precio de las propiedades: 

* Modelo **Superficie y Habitaciones**

* Modelo **Superficie y Tipo de Propiedad**

* Modelo **Superficie y Barrios**

* Modelo **Superficie y Tipo de barrio**

Agregamos tres adicionales: 

* **Modelo varias**: Superficie cubierta, descubierta, habitaciones y tipo de propiedad. 

* **Modelo varias con barrios**: Superficie cubierta, descubierta, barrios, habitaciones y tipo de propiedad. 

* **Modelo varias con tipo de barrio**: Superficie cubierta, descubierta, barrios, habitaciones y tipo de propiedad. 

Volvemos a realizar estos modelos utilizando el dataset de entrenamiento:

```{r}
# Modelo de superficie y  Habitaciones
modelo_sc_r <- lm(price ~ surface_covered + rooms, data = train_data)
# Modelo de superficie y tipo de propiedad
modelo_sc_pt <- lm(price ~ surface_covered + property_type, data = train_data)
# Modelo de superficie y barrios
modelo_sc_l3 <- lm(price ~ surface_covered + l3, data = train_data)
# Modelo de superficie y tipo de barrio
modelo_sc_tb <- lm(price ~ surface_covered + tipo_barrio, data = train_data)
# Modelo varias 
modelo_varias <- lm(price ~ surface_covered + surface_uncovered + rooms + property_type, data = train_data)
# Modelo varias + barrios
modelo_varias_l3 <- lm(price ~ surface_covered + surface_uncovered + rooms + property_type + l3, data = train_data)
# Modelo varias + tipo de barrio
modelo_varias_tb <- lm(price ~ surface_covered + surface_uncovered + rooms + property_type + tipo_barrio, data = train_data)
```

En el notebook previo sólo habíamos interpretado el valor de los parámetros estimados y su nivel de significancia. Ahora buscaremos responder preguntas tales como:

¿Qué proporción de la variabilidad logra explicar el modelo? ¿Cómo decidir que modelo explica mejor el fenómeno?

¿El modelo cumple con los supuestos del modelo lineal?

## Evaluación del Modelo 

Utilizando el paquete broom, vamos a analizar las medidas de resumen del modelo y graficamos coeficientes estimados. Mostramos el ejemplo para algunos de estos modelos.

**Modelo Superficie y Tipo de Propiedad**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_sc_pt <- tidy(modelo_sc_pt, conf.int = TRUE) %>% arrange(p.value)
tidy_sc_pt
# Plot de los Coeficientes
ggplot(tidy_sc_pt, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen",size=2) +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen", size=1) +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```
En este gráfico podemos observar los coeficientes estimados para este modelo con sus respectivos intervalos de confianza. Tanto en el gráfico como en la salida del modelo tidy se puede apreciar que el intervalo de confianza (IC) del 95% de las variables no contienen al 0. Es decir, que tanto la superficie cubierta como los tipos de propiedades resultan significativas para explicar el precio. 

**Modelo Superficie y Barrios**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_sc_l3 <- tidy(modelo_sc_l3, conf.int = TRUE) %>% arrange(p.value) 
tidy_sc_l3 
# Plot de los Coeficientes
ggplot(tidy_sc_l3, aes(estimate, term, color=p.value < 0.05, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point() +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh() +
  scale_color_manual(values=c('firebrick', 'forestgreen')) +
  guides(color="none") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```
En el caso de la variable barrios, este test permite chequear si los valores medios del precio son los mismos para los distintos barrios respecto del Abasto (categoría basal). En este caso se observa que los niveles medios de precio de inmuebles en los distintos barrios en algunos casos no difieren de los niveles medios del Abasto (tienen p-valor > 0.05 y sus IC incluyen al 0).

**Modelo Superficie y Tipo de barrio**

```{r}
# medidas de resumen tidy (incluido el intervalo de confianza)
tidy_sc_tb <- tidy(modelo_sc_tb, conf.int = TRUE) %>% arrange(p.value)
tidy_sc_tb
# Plot de los Coeficientes
ggplot(tidy_sc_tb, aes(estimate, term, xmin = conf.low, xmax = conf.high, height = 0)) +
  geom_point(color = "forestgreen") +
  geom_vline(xintercept = 0, lty = 4, color = "black") +
  geom_errorbarh(color = "forestgreen") +
  theme_bw() +
  labs(y = "Coeficientes β", x = "Estimación")
```
¿Cómo se interpretan estos coeficientes?

Podemos calcular esta información para todos los modelos juntos, usando la función map_df de la librería purrr para poder mostrar todas las salidas en un mismo dataframe.

```{r}
# armamos lista con todos los modelos
models <- list(modelo_sc_r = modelo_sc_r, modelo_sc_pt = modelo_sc_pt, modelo_sc_l3 = modelo_sc_l3, modelo_sc_tb = modelo_sc_tb, modelo_varias = modelo_varias, modelo_varias_l3 = modelo_varias_l3, modelo_varias_tb = modelo_varias_tb)
# calculamos las variables resumen
map_df(models, tidy, .id = "model")
```

### Coeficientes de determinación $R^2$ y $R^2$ ajustado

$$ R^2 = 1 − \frac{SSRes}{SSTot} = \frac{SSReg}{SSTot} $$

El $R^2$ permite medir el porcentaje de variabilidad del fenómeno que el modelo logra explicar. 

Sin importar la relevancia de la/s variables regresoras, el $R^2$ aumenta al agregar una variable adicional al modelo, aunque no se incremente la capacidad explicativa. Es decir, que podríamos utilizarlo para comparar modelos con igual número de variables, pero en casos de modelos con distinto número no serían comparables. En estos casos conviene comparar modelos por medio del $R^2_{a}$ (ajustado) que incluyen justamente el número de variables en el modelo. 

$$ R^2_{a,p} = 1 − \frac{(\frac{SSRes_p}{n − p})}{(\frac{SSTot}{n − 1})} = 1 - (\frac{n-1}{n-p})(\frac{SSRes_p}{SSTot}) $$
Como $SSTot/(n−1)$ está fijo en un conjunto de datos dado (sólo depende de las Y observadas), el $R^2_a$ aumenta si y sólo si la $SSRes_p/(n-p)$ disminuye.

Veamos qué pasa con los ajustes que hicimos. 

```{r}
# calculamos las métricas para todos los modelos
df_evaluacion_train = map_df(models, glance, .id = "model") %>%
  # ordenamos por R2 ajustado
  arrange(desc(adj.r.squared))

df_evaluacion_train
```

### Selección del mejor modelo en training

¿Cuál es el modelo que mejor explica la variabilidad del conjunto?

Como no todos los modelos tienen igual número de variables, debemos compararlos a través del $R^2_a$, ya que sino podemos incurrir en un error. Por ejemplo, vemos que en los modelos que incluyen a l3, el $R^2$ es mayor al $R_a^2$.
Por lo expuesto previamente esa diferencia podría estar explicada por el mayor número de dummies que contienen. A pesar de ello los mejores modelos resultan ser los que incluyen la variable barrios ya que poseen mayor $R_a^2$.

Analizando el R2 ajustado, el **mejor modelo** resulta ser modelo_varias_l3 que explica el precio en función de superficie cubierta, descubierta, barrios, habitaciones y tipo de propiedad. Este modelo explica 78,20% de la variabilidad del precio, es decir, más que todos los restantes modelos. 


## Diagnóstico de Modelos

El diagnóstico del modelo consiste en utilizar técnicas para validar el cumplimiento (o no) de los supuestos del modelo lineal. Recordemos que estos supuestos se puede resumir en:

$ε_i ∼ N(0,σ^2)$ independientes entre sí.

Los errores tienen distribución normal con media cero y varianza constante y son independientes entre sí. Los errores son inobservables, por lo tanto, tendremos que trabajar con su correlato empírico: los residuos.

Vamos a efectuar el diagnóstico de dos de los mejores modelos según el R-cuadrado ajustado. 

**Diagnóstico para el Modelo Varias con Barrios**

```{r}
plot(modelo_varias_l3)
```

* *Residuos vs valores predichos*: Parece existir cierta estructura en los datos: la varianza parece incrementarse con los valores predichos (depende de la variable), por lo que no se satisface el supuesto de homocedasticidad.

* *Normal QQ plot*: El extremo superior derecho e izquierdo inferior no se ajustan a la distribución teórica.

* *Residual vs leverage*: Existen algunos puntos con un leverage bastante alto.

* *Diagnóstico del modelo*: El modelo creado no cumple con los supuestos del modelo lineal. Parecen existir problemas de heterocedasticidad (varianza no constante), falta de normalidad y presencia de observaciones de alto leverage.

Veamos la versión tidy de estos gráficos. Para ello creamos un dataframe con la información de los distintos modelos creados. 

```{r}
# calculamos valores predichos para todos los modelos
au_modelos = map_df(models, augment, .id = "model")
# observamos lo que ocurre con las variables que no se incluyen en el modelo
au_modelos %>%
  head(5)
au_modelos %>%
  tail(5)
```

Al contar con esta información como dataframe, podemos realizar los gráficos de diagnóstico con ggplot. Vamos a ver dos ejemplos con los mejores modelos, pero se podría hacer para todos el mismo ejercicio. 

```{r}
# Modelo varias con barrios
g1 = ggplot(au_modelos %>% filter(model == "modelo_varias_l3"), 
       aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE) +
  labs(title = "Residuos vs valores predichos") + 
  theme_bw()
g2 = ggplot(au_modelos %>% filter(model == "modelo_varias_l3"), 
       aes(sample = .std.resid)) +
  stat_qq() +
  geom_abline() +
  labs(title = "Normal QQ plot") + 
  theme_bw()
g3 = ggplot(au_modelos %>% filter(model == "modelo_varias_l3"), 
       aes(.fitted, sqrt(abs(.std.resid)))) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Scale-location plot")
g4 = ggplot(au_modelos %>% filter(model == "modelo_varias_l3"), 
       aes(.hat, .std.resid)) +
  geom_vline(size = 2, colour = "white", xintercept = 0) +
  geom_hline(size = 2, colour = "white", yintercept = 0) +
  geom_point() + 
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Residual vs leverage")
# grafico todos juntos
grid.arrange(g1, g2, g3, g4, nrow = 2)
```

También podemos identificar cuáles son los datos con alto leverage.

```{r}
au_modelos %>% filter(model == "modelo_varias_l3") %>%
  filter(.hat>0.4)
```


**Diagnóstico para el Modelo Varias sin incluir barrio**

```{r}
plot(modelo_varias)
```

Calculamos los mismos gráficos con ggplot. 

```{r}
# Modelo varias sin barrios
g1 = ggplot(au_modelos %>% filter(model ==  "modelo_varias"), 
       aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(se = FALSE) +
  labs(title = "Residuos vs valores predichos") + 
  theme_bw()
g2 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(sample = .std.resid)) +
  stat_qq() +
  geom_abline() +
  labs(title = "Normal QQ plot") + 
  theme_bw()
g3 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(.fitted, sqrt(abs(.std.resid)))) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Scale-location plot")
g4 = ggplot(au_modelos %>% filter(model == "modelo_varias"), 
       aes(.hat, .std.resid)) +
  geom_vline(size = 2, colour = "white", xintercept = 0) +
  geom_hline(size = 2, colour = "white", yintercept = 0) +
  geom_point() + 
  geom_smooth(se = FALSE) + 
  theme_bw() +
  labs(title = "Residual vs leverage")
# grafico todos juntos
grid.arrange(g1, g2, g3, g4, nrow = 2)
```

* *Residuos vs valores predichos*: Parece existir cierta estructura en los datos: la varianza parece incrementarse con los valores predichos y luego se reduce (depende de la variable), por lo que no se satisface el supuesto de homocedasticidad.

* *Normal QQ plot*: El El extremo superior derecho e izquierdo inferior no se ajustan a la distribución teórica, por lo que no parecen seguir una distribución normal.

* *Residual vs leverage*: Existe un punto con un leverage más alto.

Veamos cuál es ese punto. 

```{r}
au_modelos %>% filter(model == "modelo_varias") %>%
  filter(.hat == max(.hat))
```

* *Diagnóstico del modelo*: El modelo creado no cumple con los supuestos del modelo lineal, dado que se detecta existencia de heterocedasticidad (varianza no constante), falta de normalidad y presencia de una observación de alto leverage.

## Evaluación en el dataset de testing

### Predicción y métrica de error

Si vamos a utilizar un modelo para predecir nuevos datos nos interesa observar el error del modelo en el dataset de **training** y en el dataset de **testing**. 

En los problemas de regresión, se suele emplear el RMSE (root mean square error) como medida del error de predicción. Una de sus ventajas es que es una métrica que está en las mismas unidades que los datos originales.

$RMSE = \sqrt{\frac{\sum(\hat{y_i}-y_i)^2}{n}}$

Primero vamos a emplear la función `augment` para predecir el precio sobre el dataset de testeo. Cuando se proporciona newdata, solo devuelve las columnas .fitted y .resid. 

Observemos el resultado para el **modelo varias con barrios**

```{r}
# Agregamos la predicciones al dataset de testeo
pred_varias_l3 = augment(modelo_varias_l3, newdata = test_data) 
pred_varias_l3 %>% select(l3,rooms, surface_covered, surface_uncovered, property_type, price, .fitted, .resid)
```

Para calcular el RMSE, vamos a utilizar la función `rmse` de la librería [**yardstick**](https://yardstick.tidymodels.org/) de **tidymodels**. Los parámetros de la función son:

  * data: dataframe con las columnas truth y estimate
  * truth: nombre de la columna con el valor de verdad
  * estimate: nombre de la columna del valor predicho

```{r}
rmse(data = pred_varias_l3, truth = price, estimate = .fitted)
```

### Comparación entre training y testing

Podemos obtener los valores del RMSE en el dataset de **training** para los modelos evaluados usando `map`.

```{r}
# Aplicamos la función augment a los modelos
lista_predicciones_training = map(.x = models, .f = augment) 
```

Creamos una lista de dataframes con los valores predichos para cada modelo. Luego podemos aplicar la función para calcular el RMSE de todos los modelos en el set de **training**.

```{r}
# Obtenemos el RMSE para los 4 modelos
map_dfr(.x = lista_predicciones_training, .f = rmse, truth = price, estimate = .fitted, .id="modelo") %>% arrange(.estimate)
```

Se observa que el modelo que obtiene el menor RMSE es el **modelo_varias_l3** (con barrios). Coincide con los resultados del R-cuadrado ajustado.

¿Qué resultado obtuvimos al observar el R-cuadrado ajustado?

```{r}
df_evaluacion_train
```

Veamos que sucede cuando obtenemos el RMSE de los modelos en el set de **testing** 

```{r}
# Aplicamos la función augment a los 4 modelos con el set de testing
lista_predicciones_testing = map(.x = models, .f = augment, newdata = test_data) 
# Obtenemos el RMSE para los 4 modelos
map_dfr(.x = lista_predicciones_testing, .f = rmse, truth = price, estimate = .fitted, .id="modelo") %>% arrange(.estimate)
```

El modelo que tiene el menor RMSE en testing continúa siendo el modelo **modelo_varias_l3**. ¿Cuál modelo elegirían para predecir nuevas observaciones?
